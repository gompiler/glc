#+TITLE: Design Document for Milestone 1
#+AUTHOR: Lore, J., Lougheed D., Wang A.
#+LATEX_HEADER: \usepackage[margin=1in]{geometry}
This document is for explaining the design decisions we had to make
whilst implementing the components for milestone 1.
* Introduction
** Implementation Language
   For our ~GoLite~ compiler, we decided to use ~Haskell~ as the
   implementation language, as many of the required tasks can be done
   more naturally (i.e. tree traversal, recursing over self defined
   structures, enforcing types, etc.).
** Tools
   For scanning, we use the program ~Alex~, which is similar to ~flex~
   but produces ~Haskell~ code instead. We use ~Happy~ for parsing,
   which resembles ~bison/yacc~. ~Alex~ and ~Happy~ can be interlinked
   nicely, passing on useful information (encompassed through a state monad)
   such as the line number, column number, character offset in file, and
   error messages (which are propagated in a pure manner
   without having to raise exceptions).

   There are several other good Haskell scanning/parsing options, such as
   ~Megaparsec~. We decided on ~Alex/Happy~ because of their syntactic
   similarity to ~flex/bison~, which we learned in class.

   We use ~stack~ to manage our Haskell project, including ~GHC~, the
   respective projects and tests, additional tests which we implemented
   using the ~hspec~ package.
** Miscellaneous
*** Continuous Integration
    We use Travis CI to build our project on every push, as well as
    run our tests to make sure that new changes did not break any
    existing functionality.
*** File Organization
    - ~src/~
      - ~generated/~
        - Code generated by ~golite.y~, using ~Happy~, and ~golite.x~, using ~Alex~
      - Modules for each feature
    - ~app/~
      - ~Main.hs~ (main, imports the needed modules and uses
        ~ParseCLI~ to parse the command line arguments and call the
        function we want)
    - ~test/~
      - ~Spec.hs~ (auto discovers ~hspec~ tests to run them all with
        ~stack test~)
      - Spec modules, each exposing a test suite
* Scanner
  The first step of this milestone was to scan user input into
  tokens. Most of the tokens we had to account for were straightforward
  and similar to the assignments. However, we had to encompass a few of
  the quirks/more complicated things relating to ~GoLite~ compared to
  something simple like ~MiniLang~.
** Semicolon Insertion
   The first hurdle of scanning was semicolon insertion. To solve
   this, we use start codes in ~Alex~ and have special rules for
   certain states.

   If the last token we scanned was something
   that can take an optional semicolon, we enter the $nl$ state, where
   encountering a newline would transform the newline into a
   semicolon. Scanning anything else (that isn't whitespace or otherwise
   ignored) returns the scanner to the default state ($0$), where
   newlines are just ignored.

   This seems to be an elegant solution, as we don't have to traverse
   the whole list or store context other than the start code, a
   built-in feature of ~Alex~. It also makes a lot more sense to let
   the scanner handle these situations automatically rather than trying
   to handle them using the parser.
** Block comment support
   Block comments cannot be easily scanned using only regular
   expressions. One potential solution was to use start codes again
   (changing state upon opening a comment), although that would
   potentially add a lot more states because we'd have to combine
   possibilities with the newline insertion state: (as block
   comments that span over newlines should also insert semicolons when
   they're optional). Another thing is that using a state approach
   made it harder to catch the error of an unclosed comment block.

   In our implemented solution, we iterate through the scanner's input
   once we receive an open comment block and ignore everything until we
   close the block comment. If the comment never closes, we can easily
   emit an error.

   To account for semicolon insertion with block comments, we set a
   semicolon flag to true if we encounter any new line inside the
   block comment, and we insert a semicolon if the start code is
   $nl$, the aforementioned newline insertion state.
*** Adding newlines at the end of the file if they aren't present already
    ~Go~ requires a semicolon or newline at the end of function
    declarations. Sometimes, there may not be a newline at the end of
    a file, so no semicolon insertion occurs.

    In order to correct for this type of file, we enforce a final
    newline by appending one if necessary. To do this, the code string
    is preprocessed prior to scanning. This was the easiest solution
    as otherwise we'd have to try and insert a semicolon if we were in
    the $nl$ state when we encounter an ~EOF~, while also making sure
    to return an ~EOF~. This would be difficult without additional
    context information.
** Nicer error messages
   We decided to use ~ErrorBundle~ from ~Megaparsec~ in order to
   output nicer error messages, with program context for easier
   debugging from an end-user programming perspective:

#+BEGIN_SRC
Error: parsing error, unexpected ) at 5:22:
  |
5 | func abstract(a, b, c) {
  |                      ^
#+END_SRC

   While we did have access to character offset, line, and column when
   generating error messages, we did not have access to the entire
   source file string, as the default behaviour of the scanner is to not
   keep the entire string at each step.

   In order to generate the contextual message, we modified the ~monad~
   wrapper provided with ~Alex~ (see ~TokensBase.hs~) and changed the
   ~Alex~ monad to wrap over a ~Either (String, Int) a~ instead of
   ~Either String a~, i.e. in addition to storing an error message on
   the left side of the monad we also carry an ~Int~ which represents
   the offset of the error, so that when we want to print the error
   message at the end we can append the part in the source file where
   the error occurred.
* Parser
** Grammar
   Many of our difficulties in the grammar were associated with identifier and
   expression lists. Two constructs in the Go (and GoLite) spec are identifier
   lists, used in declarations and function signatures, and expression lists,
   used in assignment and function signatures. The grammar was refactored to
   avoid this problem by allowing identifier lists to become expression lists
   if needed in a way which avoided introducing other conflicts.

   Another difficulty we had was with list ordering. LR parsers
   work more intuitively with rules that put the newly-created terminal
   after the recursively-expanding non-terminal However, since Haskell
   uses recursive lists defined in the opposite way, it is significantly
   faster to prepend items rather than append them. Although more
   performant, this prepending results in a reversed ordering, which
   must be handled after the list is 'complete'.

   We initially wanted to avoid reversing the lists ourselves with each usage.
   However, adding an extra non terminal to manage reversals for each list made
   our generated module notably more complex and needlessly increased our
   grammar size, so we decided against it.

   Our exact solution was to differentiate between a list which contained at
   least one non-identifier (i.e. either all non-identifiers, a list of
   identifiers plus one non-identifier, or a mixed list as the grammar rule
   base cases) and a list of entirely identifiers. Then, the expression list
   non-terminal was allowed to yield either one of these mixed lists or a
   pure identifier list.

   Another caveat of how lists are handled in the grammar, again a
   compromise to prevent ambiguity, is that the actual grammar constructs
   that represent lists correspond to a list of size two or more, which
   doesn't exactly match the Go spec (where a list may be 0/1 or more,
   depending on the case). The actual single-item non-terminals in some
   contexts / rules represent a list of size one. However, this disparity
   is resolved in the actual AST construct, which is closer to a direct
   representation of the Go / GoLite specifications.
** AST
   The AST is largely a one to one mapping of the Golang specs, with
   parts we don't support removed and additional parts for Golite added.

   In some cases, there are minor deviations from the CFG.
*** Accurate Type Representation
    We modeled our AST as close as possible to the actual Go and
    GoLite specs, to try and ensure that impossible states are inherently
    prevented by the Haskell type checker, reducing run-time errors.
    Although we don't have type-checking implemented at this milestone,
    we can use this technique to enforce definitions such as
    'exactly one', 'one or more', and 'zero or one'. This modeling is
    not always perfect. For example, a [[https://golang.org/ref/spec#IdentifierList][list]]
    of identifiers is 'one or more' (in Haskell, ~NonEmpty~). Many locations
    make it optional. While a direct translation would be ~Maybe (NonEmpty a)~,
    we choose to make it a possibly empty list ~[a]~ as it makes more sense.
*** Simplified Data Type Categories
    Some splits, such as ~add_op~ and ~mul_op~ are distinguished
    purely to demonstrate precedence; they are in fact only used once
    in the specs, so we decide to merge them directly in our ~ArithmOp~
    model. Several other instances exist.

    Given we are creating an AST, rather than a CST, we can further
    compact parts of the grammar. For instance, an ~if~ clause in the
    spec leads to an ~IfStmt~ construction, whose ~else~ body is either
    a block (with surrounding braces) or another ~if~ statement (no
    surrounding braces). In our case, we don't need to model the braces,
    so we can treat the ~else~ body exclusively as ~Stmt~ rather than
    the more verbose ~Either Block IfStmt~. The grammar enforces that
    this ~Stmt~ is not any other type.
*** Structure Simplification
    For ~var~ and ~type~ declaration, we make no distinction between
    single declaration (exactly one) and block declaration (0 or
    more). Unlike types, which produce different formats, we decide to
    enforce all declarations of one var to be single declaration. In
    other words, ~var ( a = 2 )~ would become ~var a = 2~. Note that
    we cannot further simplify group declarations ~var ( a, b = 2,
    3)~, as there is no guarantee at this stage that the number of
    identifiers matches the number of values. This would have to be
    checked at a later stage
** Weeding
    In our first stage, our weeding operations are simple, and don't rely
    on context outside of the statement we are verifying. As a result,
    we were able to define recursive traversal methods to verify relevant
    statements, and create verifiers that validate at a single level.
    Haskell helped immensely here, as we were able to use pattern matching
    to produce performant and independent functions.
    Each verifier returns an optional error, and we are able to map the results
    and return the first error, if any.
* Pretty Printer
  When creating our pretty printer, we chose a top down approach.
  Every node has the ability to output a list of strings, which makes
  it easier to format indentation. Each node is also only concerned
  with its respective subtree, and does not require context from its
  parent. We focused on aesthetics, focusing on proper spacing and
  alignments. In the case of expressions, we tried to add brackets
  sparingly, though further optimizations can be done down the road
  (a nested binary op does not always need brackets, if the order of
  precedence matches). To produce the full program, we simply join
  the list of strings in the full program, intercalated with new lines.
* Team
** Team Organization
   We started the project by dividing the main components (scanner, parser,
   AST/weeding) among the three group members (Julian, David, and Allan
   respectively). We used GitHub's organization features, such as issues
   and pull requests/code reviews, extensively in order to keep track of
   design goals, report bugs, and keep code quality as high as possible.
** Contributions
- *Julian Lore:* Wrote the majority of the scanner and handled weird
   cases, wrote a large amount of valid/invalid programs, implemented
   many other tests (~hspec~ or small tests in our program) and looked
   over the parser, contributing a few things to it as well.
- *David Lougheed:* Wrote the bulk of the parser grammar and contributed to
   the weeder. Also wrote 3 of the valid programs and 8 of the
   invalid ones and had minor contributions to miscellaneous other components.
   Contributed to the testing of the parser and pretty printer.
- *Allan Wang:* Created the AST and helper classes for pretty printing
   and error handling.  Wrote the base package for testing as well as
   some of the embedded test cases within ~hspec~.  Added integrations
   (Travis + Slack), and gave code reviews to the other components.
