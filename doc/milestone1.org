#+TITLE: Design Document for Milestone 1
This document is for explaining the design decisions we had to make
whilst implementing the components for milestone 1.
* Introduction
** Implementation Language
   For our ~GoLite~ compiler, we decided to use ~Haskell~ as the
   implementation language, as many of the required tasks can be done
   more naturally (i.e. tree traversal, recursing over self defined
   structures, enforcing types, etc.).
** Tools
   For scanning, we use the program ~Alex~, which is very similar to ~flex~
   but produces ~Haskell~ code instead. Additionally, we use ~Happy~ for
   parsing which resembles ~bison/yacc~. ~Alex~ and ~Happy~ can be interlinked
   quite nicely together, passing on useful information (encompassed
   through a state monad), like line number, column number, offset in
   file and error messages (which can be propagated in a pure manner
   without having to raise exceptions).

   There are several other good scanning/parsing options to use with
   Haskell, such as ~Megaparsec~, however, we stuck with ~Alex/Happy~ because
   they are most similar to ~flex/bison~ that we learned in class and most
   of us we're familiar with the syntax.

   We use ~stack~ to manage our Haskell project, including ~GHC~, the
   respective projects and tests, additional tests which we implemented
   using the ~hspec~ package.
** Miscellaneous
*** Continuous Integration
    We used Travis CI to build our project on every push, as well as
    run our tests to make sure that pushes kept things working.
*** File Organization
    - ~src/~
      - ~generated/~
        - ~Tokens.hs~ (file generated by ~golite.x~ using ~Alex~)
        - ~Parser.hs~ (file generated by ~golite.y~ using ~Happy~)
      - ~Scanner.hs~ (helper functions and custom definitions to help
        with scanning, using functions from ~Tokens.hs~)
      - ~Data.hs~ (file defining AST data types)
      - ~ParseCLI.hs~ (file for parsing command line arguments by
        making subparsers, auto-produces help pages and more)
    - ~app/~
      - ~Main.hs~ (main, imports the needed modules and uses
        ~ParseCLI~ to parse the command line arguments and call the
        function we want)
    - ~test/~
      - ~Spec.hs~ (auto discovers ~hspec~ tests to run them all with
        ~stack test~)
      - ~TokensSpec.hs~ (tests for tokens/scanning)
      - ~PrettifySpec.hs~ (tests for pretty printing)
      - ~Base.hs~ (base module helper for test modules)
* Scanner
  The first step of this milestone was to scan user input into
  tokens. Most of the tokens we had to account for were straightforward
  and similar to the assignments, however we had to encompass a few of
  the quirks/more complicated things relating to ~GoLite~ compared to
  something simple like ~MiniLang~.
** Semicolon Insertion
   The first hurdle of scanning was semicolon insertion. We first
   thought of doing a second pass of the token list resulting from the
   scan, but this seemed expensive and because of our types, harder
   than it looked at first glance.

   Our scanner is encompassed completely by ~alexMonadScan~
   and is interwoven with our parser with the ~lexer~ function in
   ~Scanner.hs~ (all information is kept in a state monad and the
   parser can request each token one at a time and do things lazily),
   therefore it was not feasible to do a second pass of the resulting tokens as
   we don't get them in list form before passing them to the
   parser. Instead, we use start codes in Alex and have
   special rules for certain states, i.e. if the last token we scanned
   was something that can take an optional semicolon, we'd enter the
   $nl$ state and encountering a newline in said state would scan the
   newline to a semicolon, otherwise scanning anything else (that
   isn't whitespace/ignored), would return to the default state ($0$),
   where newlines are just ignored. This seems to be a nice/elegant
   solution as we don't have to traverse the whole list or get any
   context of any sort, other than the start code which is a feature
   built in to Alex. It also makes a lot more sense to let the scanner
   handle such situations and to replace newlines by semicolons in
   certain scenarios (rather than arbitrarily inserting semicolons
   when given some situations).
** Block comment support
   Block comments cannot be easily scanned using only regular
   expressions. One potential solution was to use start codes again
   (changing state upon opening a comment), although that would
   potentially add a lot more states because we'd have to combine
   possibilities with the newline insertion state (because block
   comments that span over newlines should also insert semicolons when
   they're optional). Another thing is that using a state approach
   made it harder to catch the error of an unclosed comment block.
   
   Therefore ~checkBlk~ was implemented to iterate through the
   scanner's input once we receive an open comment block and ignore
   everything until we close the block comment and if we never reach a
   close then we can easily emit an error.

   In addition, we had to account for semicolon insertion with block
   comments, which we were able to do by adding a new case in
   ~checkBlk~ that would set a semicolon flag to true if it
   encountered any new line in the characters inside the block comment
   and then we were able to insert a semicolon if the start code was
   $nl$, which was conveniently available for us.
*** Adding newlines at the end of the file if they aren't present already
    ~Go~ adds a semicolon at the end of a file even if there's no
    newline at the end of the file for semicolon insertion. In order
    to enforce this, we simply make sure every program scanned ends
    with a newline, by appending a newline if it doesn't already end
    with a newline (preprocess string input before scanning). This was
    the easiest solution as otherwise we'd have to try and insert a
    semicolon if we were in the $nl$ state when we encounter an ~EOF~
    (but we have to return an ~EOF~ and couldn't return a semicolon
    and an ~EOF~).
*** Niche Cases
    Floats accepted by ~GoLite~ are a superset of the floats accepted
    by ~Haskell~, so we couldn't just read in the float. \(1.\) and
    \(.1\) wouldn't be recognized as floats and so we append a $0$ on
    the side that doesn't have any numbers to ensure our program can
    read the float.

    Escape characters for literal runes had to be accounted for
    separately compared to normal strings because to extract the rune
    we extract the ~Char~ in between the ~' '~, but escape characters
    after scanning are two ~Char~ s, i.e. ~\~ and something else. So
    we matched on the input of these cases and would return the
    respective escape sequence requested.
** Nicer error messages
   We decided to use ~ErrorBundle~ from ~Megaparsec~ in order to
   output nicer error messages. While we did have access to offset,
   line and column when generating error messages, we did not have
   access to the entire source file string (the scanner would not keep
   it at each step). So, in order to generate the contextual part of
   the source file showing where the error is, we modified the ~monad~
   wrapper provided with ~Alex~ (see ~TokensBase.hs~) and changed the
   ~Alex~ monad to wrap over a ~Either (String, Int) a~ instead of
   ~Either String a~, i.e. instead of just an error message on the
   left side we also carry an ~Int~ which represents the offset of the
   error so that when we want to print the error message at the end we
   can append
   the part in the source file where the error occured.
* Parser
** Grammar
   Certain productions match a list and an item of that list,
   resulting in a new list. Because our lists our immutable, we
   decided to prepend the items to our list and reverse them in
   another production that uses said list, as appending each time
   would have been a lot more costly in terms of speed.

   TODO: Issues with reversing, original solution and realization about
   non-terminals...
** AST
   The AST is largely a one to one mapping of the Golang specs, with
   parts we don't support removed and additional parts for Golite added.

   In some cases, there are minor deviations from the CFG.
*** Accurate Type Representation
    We model our ast as accurately as possible, such that impossible
    states are forbidden. We lack any checks for compatible types at
    this stage, but we can match the definition for 'exactly one', 'one
    or more', and 'zero or one'. In cases like identifiers, a [[https://golang.org/ref/spec#IdentifierList][list]] is
    one or more (haskell ~NonEmpty~), yet many locations make it
    optional. While a direct translation would be ~Maybe (NonEmpty a)~,
    we choose to make it ~[a]~ as it makes more sense.
*** Simplified Data Type Categories
    Some splits, such as ~add_op~ and ~mul_op~ are distinguished
    purely to demonstrate precedence. They are in fact only used once,
    so we decide to merge them directly in our ~ArithmOp~
    model. Several other instances exist.

    Given we created an AST vs a CST, we can further compact parts of
    the grammar. For instance, and if clause in the spec leads to an
    ~IfStmt~ grammar, whose ~else~ body is either a block (with
    surrounding braces) or another if statement (no surrounding
    braces). However, in our case, we don't need to model the braces,
    so we can treat the else body exclusively as ~Stmt~ vs ~Either
    Block IfStmt~
*** Format Preservation
    By design, our types for ~int~ and ~string~ specify whether they
    are hex/octal/dec or raw/interpreted respectively. We kept this
    information so that our pretty print would accurately represent
    the input, even though we can convert them all to a single type
    (eg dec and interpreted)
*** Structure Simplification
    For var and type declaration, we make no distinction between
    single declaration (exactly one) and block declaration (0 or
    more). Unlike types, which produce different formats, we decide to
    enforce all declarations of one var to be single declaration. In
    other words, ~var ( a = 2 )~ would become ~var a = 2~. Note that
    we cannot further simplify group declarations ~var ( a, b = 2,
    3)~, as there is no guarantee at this stage that the number of
    identifiers matches the number of values. This would have to be
    checked at a later stage
** Weeding
   TODO
* Pretty Printer
* Team
** Team Organization
   TODO
** Contributions
   *Julian Lore* Wrote the majority of the scanner and handled weird
   cases, wrote a large amount of valid/invalid programs, implemented
   many other tests (~hspec~ or small tests in our program) and looked
   over the parser, contributing a few things to it as well.

   *David Lougheed:* I wrote the bulk of the parser grammar and contributed to
   the weeder. Additionally, I wrote 3 of the valid programs and 8 of the
   invalid ones and had minor contributions to miscellaneous other components.

   *Allan Wang* TODO
