
# Table of Contents

1.  [Introduction](#org6456dc2)
    1.  [Implementation Language](#org8e517b4)
    2.  [Tools](#org4b58c0e)
    3.  [Miscellaneous](#orga195a26)
        1.  [Continuous Integration](#org58d1b70)
        2.  [File Organization](#org46dab3a)
2.  [Scanner](#org181cfe2)
    1.  [Semicolon Insertion](#org1a259b3)
    2.  [Block comment support](#org454f046)
        1.  [Adding newlines at the end of the file if they aren't present already](#org4b28057)
        2.  [Niche Cases](#orgd4fc8a5)
3.  [Parser](#org317b083)
    1.  [Grammar](#orga4676d2)
    2.  [AST](#org4db66d9)
        1.  [Accurate Type Representation](#org668e4b1)
        2.  [Simplified Data Type Categories](#org8643b4a)
        3.  [Format Preservation](#org8322498)
        4.  [Structure Simplification](#org819d300)
    3.  [Weeding](#orgda6bc9c)
4.  [Pretty Printer](#orgdddc581)
5.  [Team](#org6bb3745)
    1.  [Team Organization](#orgda50f3c)
    2.  [Contributions](#org49b7340)

This document is for explaining the design decisions we had to make
whilst implementing the components for milestone 1.


<a id="org6456dc2"></a>

# Introduction


<a id="org8e517b4"></a>

## Implementation Language

For our `GoLite` compiler, we decided to use `Haskell` as the
implementation language, as many of the required tasks can be done
more naturally (i.e. tree traversal, recursing over self defined
structures, enforcing types, etc.).


<a id="org4b58c0e"></a>

## Tools

For scanning, we use the program `Alex`, which is very similar to `flex`
but produces `Haskell` code instead. Additionally, we use `Happy` for
parsing which resembles `bison/yacc`. `Alex` and `Happy` can be interlinked
quite nicely together, passing on useful information (encompassed
through a state monad), like line number, column number, offset in
file and error messages (which can be propagated in a pure manner
without having to raise exceptions).

There are several other good scanning/parsing options to use with
Haskell, such as `Megaparsec`, however, we stuck with `Alex/Happy` because
they are most similar to `flex/bison` that we learned in class and most
of us we're familiar with the syntax.

We use `stack` to manage our Haskell project, including `GHC`, the
respective projects and tests, additional tests which we implemented
using the `hspec` package.


<a id="orga195a26"></a>

## Miscellaneous


<a id="org58d1b70"></a>

### Continuous Integration

We used Travis CI to build our project on every push, as well as
run our tests to make sure that pushes kept things working.


<a id="org46dab3a"></a>

### File Organization

-   `src/`
    -   `generated/`
        -   `Tokens.hs` (file generated by `golite.x` using `Alex`)
        -   `Parser.hs` (file generated by `golite.y` using `Happy`)
    -   `Scanner.hs` (helper functions and custom definitions to help
        with scanning, using functions from `Tokens.hs`)
    -   `Data.hs` (file defining AST data types)
    -   `ParseCLI.hs` (file for parsing command line arguments by
        making subparsers, auto-produces help pages and more)
-   `app/`
    -   `Main.hs` (main, imports the needed modules and uses
        `ParseCLI` to parse the command line arguments and call the
        function we want)
-   `test/`
    -   `Spec.hs` (auto discovers `hspec` tests to run them all with
        `stack test`)
    -   `TokensSpec.hs` (tests for tokens/scanning)
    -   `PrettifySpec.hs` (tests for pretty printing)
    -   `Base.hs` (base module helper for test modules)


<a id="org181cfe2"></a>

# Scanner

The first step of this milestone was to scan user input into
tokens. Most of the tokens we had to account for were straightforward
and similar to the assignments, however we had to encompass a few of
the quirks/more complicated things relating to `GoLite` compared to
something simple like `MiniLang`.


<a id="org1a259b3"></a>

## Semicolon Insertion

The first hurdle of scanning was semicolon insertion. We first
thought of doing a second pass of the token list resulting from the
scan, but this seemed expensive and because of our types, harder
than it looked at first glance.

Our scanner is encompassed completely by `alexMonadScan`
and is interwoven with our parser with the `lexer` function in
`Scanner.hs` (all information is kept in a state monad and the
parser can request each token one at a time and do things lazily),
therefore it was not feasible to do a second pass of the resulting tokens as
we don't get them in list form before passing them to the
parser. Instead, we use start codes in Alex and have
special rules for certain states, i.e. if the last token we scanned
was something that can take an optional semicolon, we'd enter the
\(nl\) state and encountering a newline in said state would scan the
newline to a semicolon, otherwise scanning anything else (that
isn't whitespace/ignored), would return to the default state (\(0\)),
where newlines are just ignored. This seems to be a nice/elegant
solution as we don't have to traverse the whole list or get any
context of any sort, other than the start code which is a feature
built in to Alex. It also makes a lot more sense to let the scanner
handle such situations and to replace newlines by semicolons in
certain scenarios (rather than arbitrarily inserting semicolons
when given some situations).


<a id="org454f046"></a>

## Block comment support

Block comments cannot be easily scanned using only regular
expressions. One potential solution was to use start codes again
(changing state upon opening a comment), although that would
potentially add a lot more states because we'd have to combine
possibilities with the newline insertion state (because block
comments that span over newlines should also insert semicolons when
they're optional). Another thing is that using a state approach
made it harder to catch the error of an unclosed comment block.

Therefore `checkBlk` was implemented to iterate through the
scanner's input once we receive an open comment block and ignore
everything until we close the block comment and if we never reach a
close then we can easily emit an error.

In addition, we had to account for semicolon insertion with block
comments, which we were able to do by adding a new case in
`checkBlk` that would set a semicolon flag to true if it
encountered any new line in the characters inside the block comment
and then we were able to insert a semicolon if the start code was
\(nl\), which was conveniently available for us.


<a id="org4b28057"></a>

### Adding newlines at the end of the file if they aren't present already

`Go` adds a semicolon at the end of a file even if there's no
newline at the end of the file for semicolon insertion. In order
to enforce this, we simply make sure every program scanned ends
with a newline, by appending a newline if it doesn't already end
with a newline (preprocess string input before scanning). This was
the easiest solution as otherwise we'd have to try and insert a
semicolon if we were in the \(nl\) state when we encounter an `EOF`
(but we have to return an `EOF` and couldn't return a semicolon
and an `EOF`).


<a id="orgd4fc8a5"></a>

### Niche Cases

Floats accepted by `GoLite` are a superset of the floats accepted
by `Haskell`, so we couldn't just read in the float. \(1.\) and
\(.1\) wouldn't be recognized as floats and so we append a \(0\) on
the side that doesn't have any numbers to ensure our program can
read the float.

Escape characters for literal runes had to be accounted for
separately compared to normal strings because to extract the rune
we extract the `Char` in between the `' '`, but escape characters
after scanning are two `Char` s, i.e. `\` and something else. So
we matched on the input of these cases and would return the
respective escape sequence requested.


<a id="org317b083"></a>

# Parser


<a id="orga4676d2"></a>

## Grammar

Certain productions match a list and an item of that list,
resulting in a new list. Because our lists our immutable, we
decided to prepend the items to our list and reverse them in
another production that uses said list, as appending each time
would have been a lot more costly in terms of speed.

TODO: Issues with reversing, original solution and realization about
non-terminals&#x2026;


<a id="org4db66d9"></a>

## AST

The AST is largely a one to one mapping of the Golang specs, with
parts we don't support removed and additional parts for Golite added.

In some cases, there are minor deviations from the CFG.


<a id="org668e4b1"></a>

### Accurate Type Representation

We model our ast as accurately as possible, such that impossible
states are forbidden. We lack any checks for compatible types at
this stage, but we can match the definition for 'exactly one', 'one
or more', and 'zero or one'. In cases like identifiers, a [list](https://golang.org/ref/spec#IdentifierList) is
one or more (haskell `NonEmpty`), yet many locations make it
optional. While a direct translation would be `Maybe (NonEmpty a)`,
we choose to make it `[a]` as it makes more sense.


<a id="org8643b4a"></a>

### Simplified Data Type Categories

Some splits, such as `add_op` and `mul_op` are distinguished
purely to demonstrate precedence. They are in fact only used once,
so we decide to merge them directly in our `ArithmOp`
model. Several other instances exist.

Given we created an AST vs a CST, we can further compact parts of
the grammar. For instance, and if clause in the spec leads to an
`IfStmt` grammar, whose `else` body is either a block (with
surrounding braces) or another if statement (no surrounding
braces). However, in our case, we don't need to model the braces,
so we can treat the else body exclusively as `Stmt` vs `Either
    Block IfStmt`


<a id="org8322498"></a>

### Format Preservation

By design, our types for `int` and `string` specify whether they
are hex/octal/dec or raw/interpreted respectively. We kept this
information so that our pretty print would accurately represent
the input, even though we can convert them all to a single type
(eg dec and interpreted)


<a id="org819d300"></a>

### Structure Simplification

For var and type declaration, we make no distinction between
single declaration (exactly one) and block declaration (0 or
more). Unlike types, which produce different formats, we decide to
enforce all declarations of one var to be single declaration. In
other words, `var ( a = 2 )` would become `var a = 2`. Note that
we cannot further simplify group declarations `var ( a, b = 2,
    3)`, as there is no guarantee at this stage that the number of
identifiers matches the number of values. This would have to be
checked at a later stage


<a id="orgda6bc9c"></a>

## Weeding

TODO


<a id="orgdddc581"></a>

# Pretty Printer


<a id="org6bb3745"></a>

# Team


<a id="orgda50f3c"></a>

## Team Organization

TODO


<a id="org49b7340"></a>

## Contributions

**Julian Lore** Wrote the majority of the scanner and handled weird
cases, wrote a large amount of valid/invalid programs, implemented
many other tests (`hspec` or small tests in our program) and looked
over the parser, contributing a few things to it as well.

**David Lougheed:** I wrote the bulk of the parser grammar and contributed to
the weeder. Additionally, I wrote 3 of the valid programs and 8 of the
invalid ones and had minor contributions to miscellaneous other components.

**Allan Wang** TODO

