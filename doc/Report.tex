\documentclass[11pt]{article}
\usepackage[margin=1 in]{geometry}
\usepackage{hyperref, titling, fancyhdr, lastpage}

\author{\textsc{Lore}, J.\\ \textsc{Lougheed} D.\\ \textsc{Wang} A.}
\date{\today}
\title{Final Report}
\hypersetup{
  pdfauthor={Lore, J., Lougheed D., Wang A.},
  pdftitle={Final Report},
  pdfkeywords={Compiler Design} {GoLite} {Gompiler} {glc} {Final Report},
  pdfsubject={Final Report},
  pdflang={English},
  bookmarks=true,
  unicode=true,
  linktoc=all
}
\pagestyle{fancy}
\lhead{McGill University\\COMP 520 Compiler Design
  \\\thetitle}
\chead{\leftmark}
\rhead{\theauthor}
\cfoot{Page~\thepage~of~\pageref{LastPage}}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}

\begin{document}
\begin{titlepage}
	\center%

	\textsc{\LARGE McGill University}\\[1.5cm]
	\textsc{\Large COMP 520~-~Compiler Design}\\[0.5cm]
	\textsc{\large Gompiler~-~glc, a GoLite Compiler}\\[0.5cm]

	\HRule~\\[0.4cm]
	{ \huge \bfseries \thetitle}\\[0.4cm]
	\HRule~\\[1.5cm]

	% \textsc{Lore}, J.\\ \textsc{Lougheed}, D.\\ \textsc{Wang}, A.\\
	\theauthor\\%
	[3cm]

	{\large \thedate}\\[2cm]

	\vfill

\end{titlepage}
\tableofcontents
\section{Introduction}~%
\subsection{Overview of Go}~%
\subsection{Overview of GoLite}~%
\subsection{Structure of Report}~%
\section{Language and Tool Choices}
\subsection{Implementation Language}
% TODO
% MENTION CODEGEN TOOLS, i.e. Krakatau
For our \texttt{GoLite} compiler, we decided to use \texttt{Haskell} as the
implementation language, as many of the required tasks can be done
more naturally (i.e.\ tree traversal, recursing over self defined
structures, enforcing types, etc.).
\subsection{Tools}
For scanning, we use the program \texttt{Alex}, which is similar to \texttt{flex}
but produces \texttt{Haskell} code instead. We use \texttt{Happy} for parsing,
which resembles \texttt{bison/yacc}. \texttt{Alex} and \texttt{Happy} can be interlinked
nicely, passing on useful information (encompassed through a state monad)
such as the line number, column number, character offset in file, and
error messages.
\subsection{Miscellaneous}
We use Travis CI to build our project on every push to GitHub, as well as
run our tests to make sure that new changes did not break
existing functionality.

There are several other good Haskell scanning/parsing options, such as
\texttt{Megaparsec}. We decided on \texttt{Alex/Happy} because of their syntactic
similarity to \texttt{flex/bison}, which we learned in class.

We use \texttt{stack} to manage our Haskell project and corresponding tests.
We implemented additional tests using the \texttt{hspec} package.

% TODO
% Do we need to mention file organization?
% \subsubsection{File Organization}
% \begin{itemize}
% \item \texttt{src/}
% \begin{itemize}
% \item \texttt{generated/} (Code generated by \texttt{golite.y}, using \texttt{Happy}, and \texttt{golite.x}, using \texttt{Alex})
% \item Modules for each feature
% \end{itemize}
% \item \texttt{app/}
% \begin{itemize}
% \item \texttt{Main.hs} (main, imports the needed modules and uses
% \texttt{ParseCLI} to parse the command line arguments and call the
% function we want)
% \end{itemize}
% \item \texttt{test/}
% \begin{itemize}
% \item \texttt{Spec.hs} (auto discovers \texttt{hspec} tests to run them all with
% \texttt{stack test})
% \item Spec modules, each exposing a test suite
% \end{itemize}
% \end{itemize}

\subsection{Target Language}~%
\section{Scanner}
\subsection{Overview}
% TODO
% ADAPT TO FINAL REPORT
The first step of this milestone was to scan user input into
tokens. Handing of most GoLite tokens was straightforward and
similar to class assignments. In other cases, we had to deal with
some of Go/GoLite's quirkiness when compared to something simple
like \texttt{MiniLang}.

\subsection{Design Decisions}
\subsubsection{Semicolon Insertion}
Semicolon insertion was the first scanning hurdle. To solve
this, we use start codes, a built in feature of \texttt{Alex}, and have
special rules for certain states.

If the last token we scan is something that can take an optional
semicolon, we enter the \(nl\) state, where newlines are transformed into
semicolons after specific tokens. Scanning anything else (that isn't
whitespace or otherwise ignored) returns the scanner to the default
state (\(0\)), where newlines are just ignored.

This solution seems elegant, as we don't have to traverse
the whole stream post-scanning or store context, other than the
start code. It also makes sense to let the scanner handle these
situations automatically, rather than trying to deal with them
in the parsing phase.
\subsubsection{Block comment support}
Block comments cannot be easily scanned using only regular
expressions. One potential solution was to use start codes again
(changing state upon opening a comment). This was ruled out since
it would potentially add a lot more start codes (i.e.\ states), as
we'd have combinatorial possibilities with the newline insertion state
(since block comments spanning newlines should also insert semicolons
when they're optional). Using a state approach would also make it
harder to catch unclosed comment block errors.

Our solution iterates through the scanner's input once we receive an
open comment block and ignore everything until the comment is closed.
If the comment never closes, we can easily emit an error.

To account for semicolon insertion with block comments, we set a
semicolon flag to true if we encounter any new line inside the
block comment, and we insert a semicolon if we're in the
aforementioned newline insertion state.
\paragraph{Adding newlines at the end of the file if they aren't present already}
\texttt{Go} requires a semicolon or newline at the end of function
declarations. Sometimes, there may not be a newline at the end of
a file, so no semicolon insertion occurs, resulting in a parsing error.

In order to correct for this type of file, we enforce a final
newline by appending one if necessary. To do this, the code string
is preprocessed prior to scanning. This is the easiest solution,
as otherwise we would have to try inserting a semicolon if we are in
the \(nl\) state when we encounter an \texttt{EOF}, while also making sure
to return an \texttt{EOF}. This would be difficult without additional
context information.
\subsubsection{Nicer error messages}
We decided to use \texttt{ErrorBundle} from \texttt{Megaparsec} in order to
output nicer error messages, with program context for easier
debugging from an end-user programming perspective:

\begin{verbatim}
Error: parsing error, unexpected ) at 5:22:
  |
5 | func abstract(a, b, c) {
  |                      ^
\end{verbatim}

With \texttt{Alex}' default behaviour, we did not have access to the entire
source file string, as it is not kept between steps. In order to
generate the contextual message, we modified the \texttt{monad} wrapper
provided with \texttt{Alex} (see \texttt{TokensBase.hs}) and changed the \texttt{Alex}
monad to wrap over a \texttt{Either (String, Int) a} instead of
\texttt{Either String a}, i.e.\ in addition to storing an error message on
the left side of the monad we also carry an \texttt{Int} which represents
the offset of the error. When we want to print the error
message, we can then append the part in the source file where
the error occurred.

\subsection{Testing}
\section{Parser}
\subsection{Overview}
% TODO
% POTENTIALLY ADD SECTION ABOUT PARSER ROLE LIKE IN SCANNER
\subsection{Design Decisions}
\subsubsection{Grammar}
% TODO
% ACCOUNT FOR COMMENTS IN M1 REPORT
Many of our difficulties in the grammar were associated with identifier and
expression lists, used in declarations/signatures and assignment/function
calls respectively. The grammar was refactored to fix this by allowing
identifier lists to become expression lists if needed, in a way which
avoided introducing other conflicts.

The first issue we encountered was with list ordering. LR parsers
work more intuitively with rules that put the newly-created terminal
after the recursively-expanding non-terminal. However, since Haskell
uses recursive lists defined in the opposite way, it is significantly
more efficient to prepend items. This prepending results in a reversed
ordering, which must be handled after the list is `complete'.

Adding an extra non terminal to manage reversals for each list would
needlessly increase our grammar and generated code size, so we decided
against it as a solution. The solution we use is to differentiate
lists containing at least one non-identifier expression (i.e.\ using either
all non-identifiers, identifiers plus one non-identifier, or otherwise mixed
lists, as grammar base cases) from lists of entirely identifiers. Then,
the expression list non-terminal is allowed to yield either a mixed list
or a pure identifier list depending on what is needed.

Another caveat of how lists are handled in the grammar, again a
compromise to prevent ambiguity, is that the actual grammar constructs
that represent lists correspond to a list of size two or more, which
doesn't exactly match the Go spec (where a list may be 0/1 or more,
depending on the case). The actual single-item non-terminals are allowed
to represent a list of size one when needed, meaning this disparity
is resolved in the actual AST construct, which is closer to a direct
representation of the Go / GoLite specifications.
\subsubsection{AST}
% TODO
% ACCOUNT FOR M1 COMMENTS
We modeled our AST as close as possible to the actual Go and
GoLite specs, to try and ensure that impossible states are inherently
prevented by the Haskell type checker, reducing run-time errors.
Although we don't have type-checking implemented at this milestone,
we can use this technique to enforce definitions such as
`exactly one', `one or more', and `zero or one'. This modeling is
not always perfect. For example, a \href{https://golang.org/ref/spec\#IdentifierList}{list}
of identifiers is `one or more' (in Haskell, \texttt{NonEmpty}). Many locations
make it optional. While a direct translation would be \texttt{Maybe (NonEmpty a)},
we choose to make it a possibly empty list \texttt{[a]} as it makes more sense.
\paragraph{Simplified Data Type Categories}
Some splits, such as \texttt{add\_op} and \texttt{mul\_op} are distinguished
purely to demonstrate precedence; they are in fact only used once
in the specs, so we decide to merge them directly in our \texttt{ArithmOp}
model. Several other instances exist.

Given we are creating an AST, rather than a CST, we can further
compact parts of the grammar. For instance, an \texttt{if} clause in the
spec leads to an \texttt{IfStmt} construction, whose \texttt{else} body is either
a block (with surrounding braces) or another \texttt{if} statement (no
surrounding braces). In our case, we don't need to model the braces,
so we can treat the \texttt{else} body exclusively as \texttt{Stmt} rather than
the more verbose \texttt{Either Block IfStmt}. The grammar enforces that
this \texttt{Stmt} is not any other type.
Some splits, such as \texttt{add\_op} and \texttt{mul\_op} are distinguished
purely to demonstrate precedence; they are in fact only used once
in the specs, so we decide to merge them directly in our \texttt{ArithmOp}
model. Several other instances exist.

Given we are creating an AST, rather than a CST, we can further
compact parts of the grammar. For instance, an \texttt{if} clause in the
spec leads to an \texttt{IfStmt} construction, whose \texttt{else} body is either
a block (with surrounding braces) or another \texttt{if} statement (no
surrounding braces). In our case, we don't need to model the braces,
so we can treat the \texttt{else} body exclusively as \texttt{Stmt} rather than
the more verbose \texttt{Either Block IfStmt}. The grammar enforces that
this \texttt{Stmt} is not any other type.
\paragraph{Structure Simplification}
For \texttt{var} and \texttt{type} declaration, we make no distinction between
single declaration (exactly one) and block declaration (0 or
more). Unlike types, which produce different formats, we decide to
enforce all declarations of one var to be single declaration. In
other words, \texttt{var ( a = 2 )} would become \texttt{var a = 2}. Note that
we cannot further simplify group declarations \texttt{var ( a, b = 2,
	3)}, as there is no guarantee at this stage that the number of
identifiers matches the number of values.
\subsubsection{Pretty Printer}
When creating our pretty printer, we chose a top down approach.
Every node has the ability to output a list of strings, which makes
it easier to format indentation. Each node is also only concerned
with its respective subtree, and does not require context from its
parent. We focused on aesthetics, focusing on proper spacing and
alignments. In the case of expressions, we tried to add brackets
sparingly, though further optimizations can be done down the road
(a nested binary op does not always need brackets, if the order of
precedence matches). To produce the full program, we simply join
the list of strings in the full program, intercalated with new lines.

\subsection{Testing}
\section{Weeder}
\subsection{Overview}
\subsection{Design Decisions}
% TODO
% M2 WEEDING
\subsubsection{Parser Weeding}
Most of the weeding operations needed are simple and don't rely
on external context. As a result, we were able to define recursive
traversal methods to verify relevant statements, and create verifiers
that validate at a single level. Haskell helped immensely here, as
we were able to use pattern matching to produce performant and
independent functions.

For verification where statement context was important (\texttt{break} and
\texttt{continue}) we made a simple modification to the recursive traversal
to avoid exploring scopes under \texttt{for} or \texttt{switch} statements
where applicable.

Each verifier returns an optional error, and we are able to map the
results and return the first error, if any.
\subsubsection{Typecheck Weeding}
We implemented additional weeding passes for certain constraints
that could be verified either at the weeding level or the typecheck
level, since in most cases it was easier to check via weeding. The
constraints we use weeding to check for this milestone are:
\begin{itemize}
\item Checking correct use of the blank identifier. It was much easier to recurse
through the AST and gather all identifiers that cannot be blank,
and then check this whole list, versus checking usage in the
type-checking pass. Additionally, because we have offsets in our
AST, we could easily point to the offending blank identifier
without having to make error messages for each specific incorrect
usage, as it is obvious what the incorrect usage is when we print
out the location of the blank identifier.
\item Ensuring non-void function bodies end in return statements. It was easier to
do a single weeding pass; otherwise, we'd have to recurse
differently on functions that have a return type versus functions
that don't have a return type during typechecking. Essentially,
we'd need context-sensitive statement typechecking, with two
different versions depending on return type. This was not deemed
worth it compared to a single weeding pass.
\item Ensuring \texttt{init} function declarations do not have any non-void return
statements. Similar to the above, it is easier to do a weeding
pass then require context-sensitive (i.e multiple different)
traversal functions given the context of the current function
declaration.
\item Ensuring any `main' or `init' function declarations do not have any
parameters or a return type. This is again a straightforward
weeding check.  It could also have been done via typecheck, but it
is slightly easier to implement with weeding passes (no symbol
table required).
\item Checking that any top-level declarations with the identifier \texttt{init} or
\texttt{main} are functions, since they are special identifiers in Go
(and GoLite) in the global scope.
\end{itemize}


\subsection{Testing}
\section{Symbol Table}
\subsection{Overview}
% TODO
\subsection{Design Decisions}
In Haskell, data structures are typically immutable, and much of the
language is designed around this. One of the main design decisions
made around the symbol table was deciding whether to go with an
immutable or mutable symbol table. In an immutable symbol table, a
new symbol table would have to be made every time a scope is added
or modified. Right away, despite this being a conceptually better
fit for the language, the potential performance degradation of
constantly re-building the symbol table becomes evident.

As a result of this performance impact, we decided on using a
mutable symbol table, with mutability supported via Haskell's
\href{https://hackage.haskell.org/package/base-4.12.0.0/docs/Control-Monad-ST.html}{ST}
monad. The constraint provides
\href{https://hackage.haskell.org/package/base-4.12.0.0/docs/Control-Monad-ST.html\#v:runST}{runST}
(which removes the \texttt{ST}, i.e.\ mutability, from an interior data
structure). This is proven to keep functions pure (see
\href{https://iris-project.org/pdfs/2018-popl-runST-final.pdf}{A Logical
Relation for Monadic Encapsulation of State by Amin Timany et
al.}). This made the \texttt{ST} monad a better choice than other monads
providing mutability (the main example being \texttt{IO}, which if used
would have resulted in all our functions after symbol table
generation being bound by \texttt{IO}, i.e.\ impure and also harder to work
with, as they are wrapped by an unnecessary monad). The trade-off of
this decision was a large increase in the difficulty of implementing
the symbol table, which made up a huge portion of the work for this
milestone.  However, once we finish using the symbol table, our
final result (a typechecked/simplified, proven-correct AST) is pure
and very easy to manipulate for the \texttt{codegen} phase in the next
milestone.

For symbol table storage, we created a new data type \texttt{Symbol} to
represent each symbol, analogous to the different type of symbols
(types, constants, functions, and variables). A \texttt{Symbol} can be one
of:
\begin{itemize}
\item \texttt{Base}: Base types (\texttt{int}, \texttt{float64}, etc.)
\item \texttt{Constant}: Constant values (only used for booleans in GoLite)
\item \texttt{Func [Param] (Maybe SType)}: Function types, with parameters and an
optional return type.
\item \texttt{Variable SType}: Declared variables, of type SType.
\item \texttt{SType SType}: Declared types (note: in Haskell, the first \texttt{SType} here is
a constructor, and the second \texttt{SType} is a data type attached to
the constructor.)  In this way, all possible symbol types are
encompassed by a single Haskell data type.
\end{itemize}

We also created a new data type \texttt{SType}, which is used to store
vital information concerning the types we define in the original
\texttt{AST}.  An \texttt{SType} can be one of:
\begin{itemize}
\item \texttt{Array Int SType}: An array with a length and a type.
\item \texttt{Slice SType}: A slice with a type.
\item \texttt{Struct [Field]}: A struct with a list of fields.
\item \texttt{TypeMap SIdent SType}: A user-defined type, with an identifier and an
underlying type, which may be recursively mapped, eventually to a
base type.
\item \texttt{PInt, PFloat64, PBool, PRune, PString}: Base types.
\item \texttt{Infer, Void}: Special \textasciitilde{}SType\textasciitilde{}s for inferred types and void return values.
\end{itemize}
In this way, all possible GoLite types, including user-defined
types, are accounted for.

All types that are left to be inferred during symbol table
generation are updated when typechecking (we infer the type of
variables and then update their value in the symbol table, to make
sure things like assignments don't conflict with the original
inferred type).
% TODO
% Do we need to mention scoping rules?
% \subsection{Scoping Rules}
% The scoping rules we used/considered are as follows:
% \begin{itemize}
% \item Function declarations: the parameters and function body are put
% in a new scope, but the function itself is declared at the
% current scope. Note that here we had to treat the function body
% as a list of statements and not a block statement, because if we
% recursed on the block statement our block statement rule would
% put the function body in a new scope. Instead, the body must be
% in the same scope as the parameters.
% \item Block statements are put into a new scope.
% \item If statements: we open a new scope, containing the simple
% statement and expression condition at the top level, and then
% other scope(s) inside for the body/bodies: one for \texttt{if}, and one
% for \texttt{else} (if there is one). If an \texttt{else} is present, the if and
% else scope are siblings.
% \item Switch statements: open a new scope for the \texttt{switch}, and another scope
% for each switch \texttt{case}. All switch case scopes are siblings.
% \item For loop: open a new scope, with optional clauses put at the top
% level (simple statements 1 and 2, and condition). The body is put
% in a nested scope.
% \end{itemize}

\subsection{Testing}
\section{Typecheck}
\subsection{Overview}
% TODO
\subsection{Design Decisions}
For type-checking, we decided on a single-pass approach, combining
combined symbol table generation and statement type-checking. This
improves performance, and is feasible as a product of GoLite's
declaration rules, which specify that identifiers must be declared
before they can be used.

The other approach we considered involved generation of a
type-annotated AST, with types of expressions contained in the AST,
so that we could get rid of \texttt{ST} mutability from the symbol table as
soon as possible (some of us did a similar thing for the assignment,
but this was mainly relevant for print statements in C codegen
needing to know the type of the expression they're printing).

We decided on doing all typechecking at the same time as symbol
table generation because type inference has to be done to generate
this new AST, and type inference requires typechecking (e.g. \texttt{"a" +
  5} has no inferred type, since the expression is undefined; we only
know this because of typechecking).

At first, we were going to generate an annotated AST only to
typecheck things that aren't expressions. However, at that point,
since we were already doing one in-depth pass of the original AST
for symbol table generation, we decided that we might as well do the
other half of typechecking in the same phase, since it seemed odd to
split typechecking between the symbol table and a separate pass. The
alternate approach may have been more feasible if type inference did
not require typechecking, but in GoLite it did not seem to make
sense. Therefore, after the one pass of our original AST, the final
result is a typechecked AST, with extremely limited type annotation.

Additionally, we decided to resolve all type mappings (except for
structs) to their base types when generating this new AST:~all the
casts/equality checks/new type usages are already validated in
typechecking, so we don't need them anymore, nor do we need the
mappings. Thus, our new AST was also able to get rid of type
declarations (except for structs).
\subsubsection{New AST}
As mentioned above, dependency on the SymbolTable results in a
dependency on the \texttt{ST} monad, which adds complexity to each
operation.  As a result, our goal after typechecking is to create a
new AST, which reflects the new constraints we enforce.  Namely:
\begin{itemize}
\item Typecheck errors are caught beforehand, so we no longer need offsets,
or error breakpoints.
\item All variables are properly typechecked, and can therefore reference an
explicit type. Each type is composed of parent types up until the
primitives.  This includes cases like function signatures, where
we can associate each parameter with a type instead of allowing
lists of identifiers to map to a single type.  In preparation for
codegen, we can then use our new AST exclusively, without any
other mutable data structures. Any additinoal information we need
can be added back into the AST, with minimal changes to models
used at previous stages.
\end{itemize}

\subsection{Testing}
% TODO
% Do we need to mention our invalid programs?
% Summary of the check in each invalid program:
% \begin{itemize}
% \item \texttt{append-diff-type.go}: Append an expression of a different type than
% the type of the expressions of the \texttt{slice}.
% \item \texttt{append-no-slice.go}: Append to something that isn't a slice.
% \item \texttt{assign-no-decl.go}: Assign to a variable that hasn't been declared.
% \item \texttt{assign-non-addressable.go}: Assign to a LHS that is a
% non-addressable field.
% \item \texttt{cast-not-base.go}: Cast to a type that isn't a base type.
% \item \texttt{dec-non-lval.go}: Decrement something that isn't an \texttt{lvalue}.
% \item \texttt{decl-type-mismatch.go}: Declare and assign variable of explicit type
% to an expression of a different type.
% \item \texttt{float-to-string.go}: Try to cast a \texttt{float} to a \texttt{string}.
% \item \texttt{for-no-bool.go}: While variant of for loop with a condition that isn't
% a bool.
% \item \texttt{func-call.go}: Function call with arguments of different type than
% function declaration arguments.
% \item \texttt{func-no-decl.go}: Calling a function that hasn't been declared.
% \item \texttt{function-already-declared.go}: Trying to declare a function that
% has already been declared.
% \item \texttt{function-duplicate-param.go}: Trying to declare function with two
% params with same name.
% \item \texttt{if-bad-init.go}: If with an init statement that does not typecheck
% (assignment of different type).
% \item \texttt{inc-non-numeric.go}: Increment an expression that doesn't resolve
% to a numeric base type.
% \item \texttt{index-not-list.go}: Index into something that isn't a slice.
% \item \texttt{index.go}: Index that does not resolve to an int.
% \item \texttt{invalid-type-decl.go}: Declare a type mapping to a type that
% doesn't exist.
% \item \texttt{no-field.go}: Using selector operator on struct that doesn't have
% the field requested.
% \item \texttt{non-existent-assign.go}: Assigning a variable to a non existent
% variable.
% \item \texttt{non-existent-decl.go}: Trying to declare a variable of a type that
% doesn't exist.
% \item \texttt{op-assign.go}: Op-assignment where variable and expression are not
% compatible with operator (i.e. \texttt{int + string})
% \item \texttt{print-non-base.go}: Trying to print a non base type.
% \item \texttt{return-expr.go}: Returning an expression of different type than the
% return type of the function.
% \item \texttt{return.go}: Return nothing from non-void function.
% \item \texttt{short-decl-all-decl.go}: Short declaration where all variables on
% LHS are already declared.
% \item \texttt{short-decl-diff-type.go}: Short declaration where already defined
% variables on LHS are not the same type as assigned expression.
% \item \texttt{switch-diff-type.go}: Type of expression of case is different from
% switch expression type.
% \item \texttt{type-already-declared.go}: Trying to define a type mapping to a
% type that already exists.
% \item \texttt{var-already-declared.go}: Trying to declare a variable that is
% already declared.
% \end{itemize}

\section{Code Generator}
\subsection{Overview}
\subsection{Design Decisions}~%
\subsection{Testing}~%
\section{Conclusion}~%
\subsection{Experience}~%
\subsection{What We'd Change}~%
\section{Contributions}
% M1 CONTRIBS BELOW
\begin{itemize}
	\item \textbf{Julian Lore:} Wrote the majority of the scanner and handled weird
	      cases, wrote a large amount of valid/invalid programs, implemented
	      many other tests (\texttt{hspec} or small tests in our program) and looked
	      over the parser, contributing a few things to it as well.
	\item \textbf{David Lougheed:} Wrote the bulk of the parser grammar and contributed to
	      the weeder. Also wrote 3 of the valid programs and 8 of the
	      invalid ones and had minor contributions to miscellaneous other components.
	      Contributed to the testing of the parser and pretty printer.
	\item \textbf{Allan Wang:} Created the AST and helper classes for pretty printing
	      and error handling.  Wrote the base package for testing as well as
	      some of the embedded test cases within \texttt{hspec}.  Added integrations
	      (Travis + Slack), and gave code reviews to the other components.
            \end{itemize}
            % M2 CONTRIBS
            \begin{itemize}
\item \textbf{Julian Lore:} Implemented weeding of blank identifiers, symbol
table generation, typecheck (aside from type inference and
expression typechecking) and submitted invalid programs.
\item \textbf{David Lougheed:} Worked on expression type-checking and type inference,
including tests. Also worked on the weeding pass for return
statements.
\item \textbf{Allan Wang:} Added data structures for error messages, and supported
explicit error checking in tests. Created the data model for
symbol table core. Added hspec tests.
\end{itemize}

\end{document}
